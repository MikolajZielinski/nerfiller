{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "from nerfiller.inpaint.rgb_inpainter import RGBInpainter, RGBInpainterXL\n",
                "from nerfiller.inpaint.lama_inpainter import LaMaInpainter\n",
                "from nerfiller.inpaint.depth_inpainter import DepthInpainter\n",
                "from nerfiller.utils.image_utils import get_inpainted_image_row\n",
                "from nerfiller.nerf.dataset_utils import parse_nerfstudio_frame\n",
                "from nerfiller.utils.mask_utils import downscale_mask\n",
                "from nerfiller.utils.mesh_utils import dilate\n",
                "\n",
                "from pathlib import Path\n",
                "\n",
                "import torch\n",
                "import mediapy\n",
                "import json"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = \"cuda:1\"\n",
                "dataset = \"billiards\"\n",
                "lora_model_path = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rgb_inpainter = RGBInpainter(device=device, lora_model_path=lora_model_path, vae_device=device)\n",
                "# rgb_inpainter = RGBInpainterXL(device=device, lora_model_path=lora_model_path, vae_device=device)\n",
                "# rgb_inpainter = LaMaInpainter(device=device, model_path=Path(\"../data/models/big-lama\"))\n",
                "\n",
                "# uncomment for depth\n",
                "# depth_inpainter = DepthInpainter(device=device, depth_method=\"zoedepth\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "indices = [31, 10, 20, 55]\n",
                "\n",
                "depth_max = 100.0\n",
                "display_width = 512\n",
                "strength = 1.0 # only set < 1 if the region under the mask is known but noisy e.g., for nerfbusters scenes\n",
                "\n",
                "data_path = Path(f\"../data/nerfstudio/{dataset}\")\n",
                "f = open(f\"{data_path}/transforms.json\")\n",
                "transforms = json.load(f)\n",
                "f.close()\n",
                "\n",
                "images = []\n",
                "masks = []\n",
                "depths = []\n",
                "Ks = []\n",
                "c2ws = []\n",
                "for i, idx in enumerate(indices):\n",
                "    image, depth, mask, c2w, K = parse_nerfstudio_frame(\n",
                "        transforms, data_path, idx, depth_max=depth_max, device=device\n",
                "    )\n",
                "    images.append(image)\n",
                "    masks.append(mask)\n",
                "    depths.append(depth)\n",
                "    Ks.append(K)\n",
                "    c2ws.append(c2w)\n",
                "\n",
                "images = torch.cat(images)\n",
                "masks = torch.cat(masks)\n",
                "depths = torch.cat(depths)\n",
                "Ks = torch.cat(Ks)\n",
                "c2ws = torch.cat(c2ws)\n",
                "\n",
                "# seed = 0\n",
                "# generator = [torch.Generator(device=device).manual_seed(seed) for seed in range(len(images))]\n",
                "generator = None\n",
                "\n",
                "mediapy.show_images(images.permute(0, 2, 3, 1).cpu(), width=display_width)\n",
                "mediapy.show_images(masks.permute(0, 2, 3, 1).cpu(), width=display_width)\n",
                "# mediapy.show_images(depths.permute(0, 2, 3, 1).cpu(), width=display_width)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Inpaint one image\n",
                "\n",
                "You can choose which settings to use for inpainting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if lora_model_path is None:\n",
                "    ps = f\"a photo of a {dataset}\"\n",
                "    pn = \"\"\n",
                "else:\n",
                "    ps = \"a photo of sks\"\n",
                "    pn = \"\"\n",
                "\n",
                "# uncomment if you want to set the positive prompt manually\n",
                "# ps = \"bunny ears\"\n",
                "# ps = \"santa claus\"\n",
                "# ps = \"farmer, overalls\"\n",
                "# ps = \"basket\"\n",
                "\n",
                "if not isinstance(rgb_inpainter, LaMaInpainter):\n",
                "    text_embeddings = rgb_inpainter.compute_text_embeddings(ps, pn)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# choose starting image\n",
                "# starting_image = None\n",
                "starting_image = images\n",
                "# starting_image = torch.from_numpy(mediapy.read_image(\"../outputs/drawing-none/grid-prior-du/2023-11-02_032615/renders/step_78000/images/image_000030.png\") / 255.0).permute(2, 0, 1)[None].to(images)\n",
                "# mediapy.show_images(starting_image.permute(0, 2, 3, 1).cpu(), width=display_width)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "s = 0\n",
                "e = images.shape[0]\n",
                "if isinstance(rgb_inpainter, LaMaInpainter):\n",
                "    # lama inpainter\n",
                "    ma = masks[s:e]\n",
                "    reference_imagein = rgb_inpainter.get_image(image=images[s:e], mask=ma)\n",
                "else:\n",
                "    # diffusion inpainter\n",
                "    dilate_iters = 0\n",
                "    dilate_kernel_size = 3\n",
                "    ma = masks[s:e]\n",
                "    for _ in range(dilate_iters):\n",
                "        ma = dilate(ma, kernel_size=dilate_kernel_size)\n",
                "    reference_imagein = rgb_inpainter.get_image(\n",
                "        text_embeddings=text_embeddings,\n",
                "        image=images[s:e],\n",
                "        mask=ma[s:e],\n",
                "        denoise_in_grid=False,\n",
                "        multidiffusion_steps=1,\n",
                "        randomize_latents=False,\n",
                "        text_guidance_scale=0.0, # modify to > 0, e.g., 7.5 or 15.0, if you want to use text CFG\n",
                "        image_guidance_scale=1.5,\n",
                "        num_inference_steps=20,\n",
                "        generator=generator,\n",
                "        use_decoder_approximation=False,\n",
                "        replace_original_pixels=True,\n",
                "        starting_image=starting_image,\n",
                "        starting_lower_bound=strength,\n",
                "        starting_upper_bound=strength,\n",
                "        # output_folder=Path(\"inpaint/\"),\n",
                "    )\n",
                "reference_imagein_row = get_inpainted_image_row(image=images[s:e], mask=ma[s:e], inpainted_image=reference_imagein, show_original=False).permute(0,2,3,1).detach().cpu()\n",
                "mediapy.show_images(reference_imagein_row, width=display_width)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# show masks and images separately\n",
                "# mediapy.show_images(reference_imagein_row[:,:512])\n",
                "# mediapy.show_images(reference_imagein_row[:,512:])\n",
                "\n",
                "# uncomment to save a reference image. then, copy it to the nerfstudio dataset folder\n",
                "# mediapy.write_image(\"reference.png\", reference_imagein.permute(0,2,3,1)[0].detach().cpu())\n",
                "# images[0:1] = reference_imagein\n",
                "# masks[0:1] = 0.0\n",
                "# images[4:5] = reference_imagein\n",
                "# masks[4:5] = 0.0\n",
                "\n",
                "# uncomment to inpaint the depth\n",
                "# with torch.no_grad():\n",
                "#     reference_depthin = depth_inpainter.get_depth(image=reference_imagein)\n",
                "# mediapy.show_images(reference_depthin.permute(0,2,3,1).detach().cpu(), width=display_width)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Inpaint multiple images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# uncomment for expanded attention\n",
                "# from nerfiller.utils.diff_utils import register_extended_attention\n",
                "# register_extended_attention(rgb_inpainter.unet)\n",
                "\n",
                "if isinstance(rgb_inpainter, LaMaInpainter):\n",
                "    # lama inpainter\n",
                "    imagein = rgb_inpainter.get_image(image=images, mask=masks)\n",
                "else:\n",
                "    # diffusion inpainter\n",
                "    denoise_in_grid = True\n",
                "    scale_factor = 0.5 if denoise_in_grid else 1.0\n",
                "    dilate_iters = 0\n",
                "    dilate_kernel_size = 3\n",
                "    im = torch.nn.functional.interpolate(images, scale_factor=scale_factor, mode=\"bilinear\")\n",
                "    ma = downscale_mask(\n",
                "        masks,\n",
                "        scale_factor=scale_factor,\n",
                "        dilate_iters=dilate_iters,\n",
                "        dilate_kernel_size=dilate_kernel_size,\n",
                "    )\n",
                "    imagein = rgb_inpainter.get_image(\n",
                "        text_embeddings=text_embeddings,\n",
                "        image=im,\n",
                "        mask=ma,\n",
                "        denoise_in_grid=denoise_in_grid,\n",
                "        multidiffusion_steps=8,\n",
                "        multidiffusion_type=\"epsilon\",\n",
                "        randomize_latents=True,\n",
                "        randomize_within_grid=False,\n",
                "        text_guidance_scale=0.0,\n",
                "        image_guidance_scale=1.5,\n",
                "        num_inference_steps=20,\n",
                "        generator=generator,\n",
                "        replace_original_pixels=True,\n",
                "        starting_image=im,\n",
                "        starting_lower_bound=strength,\n",
                "        starting_upper_bound=strength,\n",
                "        # output_folder=Path(\"inpaint_grid/\"),\n",
                "    )\n",
                "imagein = torch.nn.functional.interpolate(imagein, scale_factor=1/scale_factor, mode=\"bilinear\")\n",
                "imagein = torch.where(masks==1, imagein, images)\n",
                "imagein_row = get_inpainted_image_row(image=images, mask=masks, inpainted_image=imagein, show_original=False).permute(0,2,3,1).detach().cpu()\n",
                "mediapy.show_images(imagein_row, width=display_width)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# show masks and images separately, nice for drag and drop to make figures\n",
                "# mediapy.show_images(imagein_row[:,:512].detach().cpu())\n",
                "# mediapy.show_images(imagein_row[:,512:].detach().cpu())\n",
                "# or, save to a folder\n",
                "# for i in range(len(imagein_row)):\n",
                "#     mediapy.write_image(f\"joint_inpainting_figure/mask_{i:0d}.png\", imagein_row[i,:512])\n",
                "#     mediapy.write_image(f\"joint_inpainting_figure/image_{i:0d}.png\", imagein_row[i,512:])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# with torch.no_grad():\n",
                "#     depthin = depth_inpainter.get_depth(image=imagein)\n",
                "# mediapy.show_images(depthin.permute(0,2,3,1).detach().cpu(), width=display_width)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # uncomment to make grid prior figure\n",
                "# prefix = \"grid_prior_figure\"\n",
                "# imagerow = get_inpainted_image_row(image=images, mask=masks, inpainted_image=reference_imagein, show_original=False).permute(0,2,3,1).detach().cpu()\n",
                "# gridimagerow = get_inpainted_image_row(image=images, mask=masks, inpainted_image=imagein, show_original=False).permute(0,2,3,1).detach().cpu()\n",
                "# for i in range(len(imagerow)):\n",
                "#     mediapy.write_image(f\"{prefix}/mask_{i:06d}.png\", imagerow[i][:512])\n",
                "#     mediapy.write_image(f\"{prefix}/individual_{i:06d}.png\", imagerow[i][512:])\n",
                "#     mediapy.write_image(f\"{prefix}/grid_{i:06d}.png\", gridimagerow[i][512:])\n",
                "\n",
                "# # uncomment to split an intermediate image separate images\n",
                "# image_filename = \"inpaint_grid/x0-000004.png\"\n",
                "# intermediate_image = mediapy.read_image(image_filename)\n",
                "# for i in range(images.shape[0]):\n",
                "#     # we assume resolution is 256\n",
                "#     intermediate_image_i = intermediate_image[:, i*256:i*256+256]\n",
                "#     mediapy.write_image(f\"{prefix}/intermediate_grid_{i:06d}.png\", intermediate_image_i)\n",
                "\n",
                "# uncomment to concat rows together for a big image\n",
                "# reference_imagein_row_cat = torch.cat(list(reference_imagein_row), dim=1).detach().cpu()\n",
                "# imagein_row_cat = torch.cat(list(imagein_row), dim=1).detach().cpu()\n",
                "# full_image = torch.cat([reference_imagein_row_cat, imagein_row_cat[512:]])\n",
                "# mediapy.show_image(full_image)\n",
                "# mediapy.write_image(f\"full_image_{dataset}.png\", full_image)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nerfiller",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.16"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
